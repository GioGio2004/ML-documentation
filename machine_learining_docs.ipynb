{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1F183Xa+cgw0aL+RfrCEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GioGio2004/ML-documentation/blob/main/machine_learining_docs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gathering information from online resources about machine learning**\n",
        "\n",
        "code examples in this documentation is gathered from **Gemini** and other scorses"
      ],
      "metadata": {
        "id": "1l5qTB764ZOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**coursera.org**\n",
        "\n",
        "Top machine learning algorithms to know\n",
        "From classification to regression, here are seven algorithms you need to know:\n",
        "\n",
        "1. **Linear regression**:\n",
        "Linear regression is a supervised learning algorithm used to predict and forecast values within a continuous range, such as sales numbers or prices.\n",
        "\n",
        "Originating from statistics, linear regression performs a regression task, which maps a constant slope using an input value (X) with a variable output (Y) to predict a numeric value or quantity.\n",
        "\n",
        "Linear regression uses labelled data to make predictions by establishing a line of best fit, or 'regression line', that is approximated from a scatter plot of data points. As a result, linear regression is used for predictive modelling rather than categorisation.\n"
      ],
      "metadata": {
        "id": "rXnnVk2D4tni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression in Machine Learning\n",
        "\n",
        "Linear regression is a fundamental supervised learning algorithm widely used for continuous variable prediction. It establishes a linear relationship between one or more independent variables (also known as features or predictors) and a dependent variable (also called target or outcome). The model learns the coefficients that best fit a straight line through the data points, enabling predictions for new unseen data.\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Supervised Learning: The model learns from labeled data where each data point has an associated target value.\n",
        "Linear Relationship: The algorithm assumes a linear association between the features and the target variable. Non-linear relationships require more advanced techniques.\n",
        "Model Fitting: The process of finding the coefficients (slope and intercept) that minimize the difference between the predicted values and the actual target values. This is often achieved using the ordinary least squares (OLS) method.\n",
        "Prediction: Once the model is trained, you can use the equation with new feature values to predict the corresponding target value.\n",
        "Code Examples\n",
        "\n",
        "Python (Scikit-learn):"
      ],
      "metadata": {
        "id": "evt58FgF9tIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "new_data = np.array([[6]])  # Example: Predict for a new feature value of 6\n",
        "predicted_value = model.predict(new_data)\n",
        "print(predicted_value)  # Output: [[5.8]] (may vary slightly due to random factors)\n",
        "\n",
        "# Get model coefficients (slope and intercept)\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "print(f\"Slope (m): {slope}, Intercept (b): {intercept}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGdGCISz5Uly",
        "outputId": "7374b9a9-7a03-405c-b3d9-1dfeef217d7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.8]\n",
            "Slope (m): 0.6, Intercept (b): 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Import libraries: numpy for numerical computations and LinearRegression from scikit-learn for the linear regression model.\n",
        "Sample data: Replace with your actual data in a NumPy array format, where X is the feature matrix and y is the target vector.\n",
        "Model creation: Instantiate a LinearRegression object.\n",
        "Model training: Call model.fit(X, y) to train the model on the provided data.\n",
        "Prediction: Use model.predict(new_data) to predict the target value for new feature values in new_data.\n",
        "Model coefficients: Access the model's slope (model.coef_[0]) and intercept (model.intercept_) to understand the linear relationship learned.\n",
        "Additional Notes\n",
        "\n",
        "Linear regression is a versatile tool for tasks like:\n",
        "Forecasting (e.g., predicting sales based on historical data)\n",
        "Trend analysis (e.g., identifying linear trends in stock prices)\n",
        "Correlation exploration (e.g., understanding the relationship between house size and price)\n",
        "It's crucial to ensure a linear relationship between features and the target variable for optimal results. Data visualization and domain knowledge can help assess this.\n",
        "Consider feature scaling for improved performance, especially when features have different units or scales.\n",
        "Linear regression may not be suitable for highly non-linear relationships. Explore other learning algorithms like decision trees, support vector machines, or neural networks for such scenarios.\n",
        "By understanding these concepts and using code examples like the one provided, you can effectively apply linear regression for continuous variable prediction in your machine learning projects."
      ],
      "metadata": {
        "id": "rfb1OIBA7CpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**additional:**\n",
        "\n",
        "**Ordinary Least Squares (OLS)**\n",
        "\n",
        "OLS is a statistical method at the heart of linear regression. It aims to find the line (or hyperplane in higher dimensions) that best fits a set of data points by minimizing the sum of squared errors (SSE). In other words, OLS searches for the coefficients (slope and intercept) that make the predicted values from the regression line as close as possible to the actual target values.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Model Equation: The equation for a linear regression model is:\n"
      ],
      "metadata": {
        "id": "shSnb6QH_LX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = b0 + b1 * x + e"
      ],
      "metadata": {
        "id": "iopp8zEx_YWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "where:\n",
        "\n",
        "y is the dependent variable (target value)\n",
        "x is the independent variable (feature)\n",
        "b0 is the intercept (y-axis value where the line crosses)\n",
        "b1 is the slope (coefficient indicating the change in y for a unit change in x)\n",
        "e is the error term (the difference between the actual value of y and the predicted value from the model)\n",
        "Minimizing SSE: OLS focuses on minimizing the sum of squared errors (SSE), which is the sum of the squared distances between the predicted and actual values of y:"
      ],
      "metadata": {
        "id": "RGs5yT9L_hBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SSE = sum((y_i - (b0 + b1 * x_i))^2)"
      ],
      "metadata": {
        "id": "0AiVFiz6_mmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "where y_i and x_i represent individual data points and the summation goes over all data points.\n",
        "\n",
        "Calculus and Solution: By taking the partial derivative of SSE with respect to b0 and b1 and setting them to zero, we can find the values of b0 and b1 that minimize SSE. This leads to a system of equations that can be solved to obtain the optimal estimates for the intercept and slope.\n",
        "\n",
        "Code Example (Python with Scikit-learn):"
      ],
      "metadata": {
        "id": "0-Bqy4Sb_oWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Model creation (OLS is built into LinearRegression)\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get model coefficients (slope and intercept)\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "print(f\"OLS Slope (m): {slope}, Intercept (b): {intercept}\")\n"
      ],
      "metadata": {
        "id": "-Ib-Gb9o_qu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "We import numpy for numerical computations and LinearRegression from scikit-learn.\n",
        "We create sample data in NumPy arrays for features (X) and target (y).\n",
        "We instantiate a LinearRegression object (which uses OLS internally).\n",
        "The model.fit(X, y) step trains the model, finding the optimal slope and intercept using OLS.\n",
        "Finally, we access the learned coefficients using model.coef_[0] (slope) and model.intercept_ (intercept).\n",
        "While Scikit-learn provides a convenient way to use OLS, understanding the underlying principle helps interpret the results and choose appropriate regression methods for your specific problems."
      ],
      "metadata": {
        "id": "TAoHSNQA_scF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LinearRegression in scikit-learn\n",
        "\n",
        "LinearRegression is a versatile class within scikit-learn's linear_model module for fitting and performing linear regression. It implements the Ordinary Least Squares (OLS) method to establish a linear relationship between one or more independent variables (features) and a continuous dependent variable (target).\n",
        "\n",
        "Key Attributes and Methods:\n",
        "\n",
        "fit(X, y): This method is the core of the model. It trains the model by fitting a linear model to the provided data (X is the feature matrix, y is the target vector). Scikit-learn employs OLS internally to find the optimal coefficients (slope and intercept) that minimize the sum of squared errors (SSE) between predicted and actual target values.\n",
        "coef_: This read-only attribute stores the estimated coefficients for the linear regression problem. For single-target regression, it's a 1D array of length n_features, representing the coefficients for each feature. For multi-target regression (passing a 2D y), it's a 2D array of shape (n_targets, n_features).\n",
        "intercept_: This read-only attribute holds the intercept (y-axis value where the regression line crosses) learned during model fitting.\n",
        "predict(X): This method enables you to predict target values for new unseen data (X) based on the fitted model. It returns a 1D array of predicted target values for each input sample in X.\n",
        "score(X, y, sample_weight=None): This method calculates the coefficient of determination (R-squared), a goodness-of-fit metric that indicates the proportion of variance in the target variable explained by the linear model. Higher R-squared values (closer to 1) suggest better model fit. The optional sample_weight argument allows weighted R-squared calculation.\n",
        "Additional Considerations:\n",
        "\n",
        "Linearity Assumption: Linear regression assumes a linear relationship between features and the target variable. If the data exhibits non-linearity, consider alternative models like decision trees, support vector machines, or neural networks.\n",
        "Feature Scaling: For improved performance, especially when features have different scales or units, feature scaling (e.g., standardization or normalization) is often recommended.\n",
        "Regularization: Techniques like L1 (LASSO) or L2 (Ridge) regularization can help address overfitting (a model that performs well on training data but poorly on unseen data) by introducing penalties for large coefficients. The Ridge and Lasso classes in scikit-learn provide options for regularized regression.\n",
        "Other Class Modules in scikit-learn:\n",
        "\n",
        "LogisticRegression: For classification tasks involving binary or multiple classes, LogisticRegression implements logistic regression to model the probability of a sample belonging to a particular class.\n",
        "Ridge and Lasso: These classes enable regularized linear regression with L2 (Ridge) and L1 (LASSO) penalties, respectively, useful for reducing overfitting and coefficient shrinkage for sparsity.\n",
        "RidgeClassifier and LassoCV: These offer classification and regression capabilities with regularization, respectively.\n",
        "LinearSVC: For Support Vector Classification (SVC) with a linear kernel, LinearSVC is efficient and suitable for high-dimensional data.\n",
        "Perceptron: This class implements the Perceptron algorithm, a simple linear classifier for binary classification.\n",
        "SGDClassifier: For Stochastic Gradient Descent (SGD) classification with various loss functions (e.g., hinge loss for linear SVC), SGDClassifier is flexible for large datasets.\n",
        "RandomizedLogisticRegression: Useful for high-dimensional sparse datasets, this class implements stochastic optimization for logistic regression.\n",
        "HuberRegressor: For robust regression less sensitive to outliers, HuberRegressor employs a smooth combination of least squares and L1 losses.\n",
        "ElasticNet: Combining L1 and L2 regularization, ElasticNet can handle sparse data with potentially correlated features.\n",
        "By understanding these classes and their capabilities, you can effectively choose the appropriate linear or regularized linear model for your specific machine learning tasks in scikit-learn.\n",
        "\n"
      ],
      "metadata": {
        "id": "xt-zcfeoBYvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression:**\n",
        "Logistic regression, or 'logit regression', is a supervised learning algorithm used for binary classification, such as deciding whether an image fits into one class.\n",
        "\n",
        "Originating from statistics, logistic regression technically predicts the probability that an input can be categorised into a single primary class. In practice, however, this can be used to group outputs into one of two categories ('the primary class' or 'not the primary class'). This is achieved by creating a range for binary classification, such as any output between 0-.49 is put in one group, and any between .50 and 1.00 is put in another.\n",
        "\n",
        "As a result, logistic regression in machine learning is typically used for binary categorisation rather than predictive modelling."
      ],
      "metadata": {
        "id": "LU_TjaX9Aq2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression for Binary Classification\n",
        "\n",
        "Logistic regression is a powerful supervised learning algorithm used for binary classification. It estimates the probability of an event (represented by the dependent variable) occurring based on one or more independent variables (features). In simpler terms, it predicts the likelihood of something belonging to one of two categories.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "Binary Classification: Logistic regression is ideal for problems where the target variable can have only two possible outcomes, typically labeled as 0 and 1 (e.g., email spam or not spam, disease present or absent).\n",
        "Logistic Function (Sigmoid): This function transforms the linear combination of features into a probability value between 0 and 1. A value closer to 1 indicates a higher probability of belonging to class 1, and vice versa.\n",
        "Model Fitting: The model learns the coefficients for the linear equation using techniques like gradient descent to minimize the difference between predicted probabilities and actual class labels.\n",
        "Code Example (Python with scikit-learn):"
      ],
      "metadata": {
        "id": "_k5yia-8BfRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])  # Features\n",
        "y = np.array([0, 1, 1, 0])  # Target labels (0 or 1)\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "new_data = np.array([[9, 10]])  # Example: Predict for new features\n",
        "predicted_probability = model.predict_proba(new_data)  # Get probabilities for both classes\n",
        "print(predicted_probability)  # Output: [[..., 0.99..., 0.01...]] (probability of class 1, probability of class 0)\n",
        "\n",
        "# Get class predictions (thresholding the probability)\n",
        "predicted_class = model.predict(new_data)\n",
        "print(predicted_class)  # Output: [1] (predicted class: 1)\n"
      ],
      "metadata": {
        "id": "HJ4sIGNYDeIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Import libraries: numpy for numerical computations and LogisticRegression from scikit-learn.\n",
        "Sample data: Replace with your actual data. X is a 2D array for features, and y is a 1D array for target labels (0 or 1).\n",
        "Model creation: Instantiate a LogisticRegression object.\n",
        "Model training: Use model.fit(X, y) to train the model on the data.\n",
        "Prediction (Probabilities): Employ model.predict_proba(new_data) to get probability values for both classes (class 1 and class 0) for new features in new_data.\n",
        "Prediction (Class Labels): Utilize model.predict(new_data) to obtain the predicted class label (0 or 1) for the new data. Here, a threshold (often 0.5) is often applied to the predicted probability to convert it to a class label (e.g., if probability >= 0.5, predict class 1).\n",
        "Remember: This is a simplified explanation for binary classification. Logistic regression can be extended to multi-class problems, but the interpretation becomes more complex.\n",
        "\n"
      ],
      "metadata": {
        "id": "4sA3P7ueDftq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here's a more complex example of logistic regression with detailed comments for each line, incorporating multi-class classification and hyperparameter tuning:\n",
        "**\n",
        "Scenario: Predicting whether an email is spam (class 1) or not spam (class 0) based on word frequencies in the email. We'll use a dataset of pre-processed emails with word frequencies as features and corresponding spam/not spam labels.\n",
        "\n",
        "Code (Python with scikit-learn):\n",
        "\n"
      ],
      "metadata": {
        "id": "nMJ60b-iDoR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load data (replace with your actual data loading method)\n",
        "data = pd.read_csv(\"email_data.csv\")\n",
        "X = data[\"text\"]  # Feature: Email text\n",
        "y = data[\"label\"]  # Target variable: Spam (1) or not spam (0)\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction: TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Extract 1000 most informative features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Logistic regression model (multi-class)\n",
        "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)  # Set solver and random state\n",
        "\n",
        "# Hyperparameter tuning (optional)\n",
        "# Use techniques like GridSearchCV for more extensive tuning\n",
        "model.set_params(C=1.0)  # Regularization parameter (example)\n",
        "\n",
        "# Model training\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediction on test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-score (weighted): {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "hOl6hWAfEm4N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Import libraries:\n",
        "\n",
        "pandas for data manipulation (assuming CSV data).\n",
        "train_test_split from sklearn.model_selection for splitting data into training and testing sets.\n",
        "TfidfVectorizer from sklearn.feature_extraction.text for feature extraction using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "LogisticRegression from sklearn.linear_model for the logistic regression model.\n",
        "accuracy_score and f1_score from sklearn.metrics for evaluation metrics.\n",
        "Data loading: Replace with your data loading method (e.g., reading CSV).\n",
        "\n",
        "X stores the email text data (features).\n",
        "y stores the class labels (spam or not spam).\n",
        "Train-test split: Splits data into training (80%) and testing (20%) sets using train_test_split.\n",
        "\n",
        "random_state=42 sets a seed for reproducibility.\n",
        "Feature extraction:\n",
        "\n",
        "TfidfVectorizer creates features by analyzing word frequencies and their importance across all emails.\n",
        "max_features=1000 limits the number of features to extract (you can experiment with different values).\n",
        "X_train_tfidf and X_test_tfidf are the transformed training and testing features (numerical representations of the text).\n",
        "Logistic regression model:\n",
        "\n",
        "LogisticRegression is instantiated with the following parameters:\n",
        "multi_class=\"multinomial\": Specifies multi-class classification (more than two classes).\n",
        "solver=\"lbfgs\": Optimization algorithm (others like \"liblinear\" might be suitable depending on dataset size).\n",
        "random_state=42: Sets a seed for reproducibility.\n",
        "Hyperparameter tuning (optional):\n",
        "\n",
        "Hyperparameter tuning involves optimizing model parameters for better performance.\n",
        "Here, we're setting the regularization parameter C (example), but consider using techniques like GridSearchCV for a"
      ],
      "metadata": {
        "id": "TM8OKuCeFOVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets get deeper into depedancies"
      ],
      "metadata": {
        "id": "z7lhV_zOFdNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YTamXq_rHaAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, train_test_split is a crucial function from the sklearn.model_selection library. It's used to split your dataset into two essential parts: training data and testing data. Here's a breakdown of its purpose and usage:\n",
        "\n",
        "What it Does:\n",
        "\n",
        "Divides your dataset into two subsets:\n",
        "Training set (usually 70-80%): This larger portion of the data is used to train your machine learning model. The model learns patterns and relationships within the training data to build its knowledge base.\n",
        "Testing set (usually 20-30%): This smaller portion remains unseen by the model during training. It's used to evaluate the model's generalizability and performance on unseen data.\n",
        "Why It's Important:\n",
        "\n",
        "Prevents Overfitting: Without splitting the data, your model might simply memorize the training examples, leading to poor performance on new data (overfitting). Testing data helps assess how well your model generalizes to unseen examples.\n",
        "Provides a Reliable Performance Estimate: By evaluating the model on unseen data (testing set), you obtain a more accurate measure of its effectiveness in real-world scenarios.\n",
        "How to Use It:\n",
        "\n",
        "The train_test_split function takes several arguments:\n",
        "\n",
        "X: The feature matrix or array containing your data samples (usually rows represent samples and columns represent features).\n",
        "y: The target vector or array containing the corresponding labels for each sample in X.\n",
        "test_size (default: 0.25): The proportion of data to allocate to the testing set (0.0 to 1.0).\n",
        "random_state (default: None): Controls the randomness for shuffling data (used for reproducibility).\n",
        "shuffle (default: True): Whether to shuffle the data before splitting.\n",
        "Here's an example of how to use train_test_split with sample data:"
      ],
      "metadata": {
        "id": "yUS0QADOFoXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n",
        "y = [1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Split data into training and testing sets (75% training, 25% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n",
        "\n",
        "print(f\"X_train: {X_train}\\nX_test: {X_test}\\ny_train: {y_train}\\ny_test: {y_test}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56RrkbMEGZQF",
        "outputId": "394f3678-2980-497c-ed49-97f4113bc588"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[1, 2], [5, 6], [7, 8], [3, 4]]\n",
            "X_test: [[11, 12], [9, 10]]\n",
            "y_train: [1, 1, 0, 0]\n",
            "y_test: [0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the code splits the data into training and testing sets with a 75% (training) to 25% (testing) ratio. You can adjust the test_size parameter to control the split ratio based on your needs.\n",
        "\n",
        "Remember, splitting your data using train_test_split is a fundamental step in any machine learning workflow. It ensures your model is trained on relevant data and generalizes well to unseen examples."
      ],
      "metadata": {
        "id": "66pkzCoiGkoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ExU3jMVWGlwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer: Transforming Text Data into Numerical Features\n",
        "\n",
        "In machine learning, particularly for text classification tasks, we often deal with textual data. However, machine learning models can't directly process text. TfidfVectorizer from sklearn.feature_extraction.text helps bridge this gap by transforming text data into numerical features suitable for model training. It employs a technique called TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "How TF-IDF Works:\n",
        "\n",
        "Term Frequency (TF): This measures how frequently a term (word) appears in a specific document.\n",
        "Inverse Document Frequency (IDF): This considers the overall importance of a term across all documents in the corpus. Words that appear frequently across all documents (e.g., \"the\", \"a\") will have lower IDF scores, while words that are specific to a few documents will have higher IDF scores.\n",
        "By combining TF and IDF, TfidfVectorizer identifies the most informative words within a document and downplays the importance of common words. This helps create a meaningful numerical representation of the text data.\n",
        "\n",
        "Code Example:"
      ],
      "metadata": {
        "id": "6B-MAh_nHkpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text documents\n",
        "documents = [\"Georgia has been inhabited since prehistoric times, hosting the world's earliest known sites of winemaking, gold mining, and textiles.[15][16] The classical era saw the emergence of several kingdoms, such as Colchis and Iberia, that formed the nucleus of the modern Georgian state. In the early fourth century, Georgians officially adopted Christianity, which contributed to their gradual unification and ethnogenesis. In the High Middle Ages, the Kingdom of Georgia reached its Golden Age during the reign of King David IV and Queen Tamar. The kingdom subsequently declined and disintegrated under the hegemony of various regional\",\n",
        "               \"Georgia has been inhabited since prehistoric times,fter the Russian Revolution in 1917, Georgia briefly emerged as an independent republic under German protectorate,[17] but was invaded and annexed by the Soviet Union in 1922, becoming one of its constituent republics. In the 1980s, an independence movement grew quickly, leading to Georgia's secession from the Soviet Union in April 1991.\"]\n",
        "\n",
        "# Create a TF-IDF vectorizer (extract 5 most informative features)\n",
        "vectorizer = TfidfVectorizer(max_features=50)\n",
        "\n",
        "# Transform documents into numerical features (TF-IDF vectors)\n",
        "features = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print feature names and their corresponding values in the TF-IDF vectors\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(f\"Feature names: {feature_names}\\nFeatures:\\n{features.toarray()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb58NyTYHn0J",
        "outputId": "f791b584-f0e3-4691-fd7c-2ee64f2fdde6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['an' 'and' 'as' 'been' 'georgia' 'has' 'in' 'inhabited' 'invaded' 'its'\n",
            " 'iv' 'king' 'kingdom' 'kingdoms' 'known' 'leading' 'middle' 'mining'\n",
            " 'modern' 'movement' 'nucleus' 'of' 'officially' 'one' 'prehistoric'\n",
            " 'protectorate' 'queen' 'quickly' 'russian' 'saw' 'secession' 'several'\n",
            " 'since' 'sites' 'soviet' 'state' 'subsequently' 'such' 'tamar' 'textiles'\n",
            " 'the' 'their' 'times' 'to' 'under' 'unification' 'union' 'various' 'was'\n",
            " 'which']\n",
            "Features:\n",
            "[[0.         0.31537198 0.0630744  0.0630744  0.12614879 0.0630744\n",
            "  0.12614879 0.0630744  0.         0.0630744  0.08864886 0.08864886\n",
            "  0.17729772 0.08864886 0.08864886 0.         0.08864886 0.08864886\n",
            "  0.08864886 0.         0.08864886 0.37844637 0.08864886 0.\n",
            "  0.0630744  0.         0.08864886 0.         0.         0.08864886\n",
            "  0.         0.08864886 0.0630744  0.08864886 0.         0.08864886\n",
            "  0.08864886 0.08864886 0.08864886 0.08864886 0.69381835 0.08864886\n",
            "  0.0630744  0.0630744  0.0630744  0.08864886 0.         0.08864886\n",
            "  0.         0.08864886]\n",
            " [0.28918488 0.10287871 0.10287871 0.10287871 0.30863614 0.10287871\n",
            "  0.41151485 0.10287871 0.14459244 0.10287871 0.         0.\n",
            "  0.         0.         0.         0.14459244 0.         0.\n",
            "  0.         0.14459244 0.         0.10287871 0.         0.14459244\n",
            "  0.10287871 0.14459244 0.         0.14459244 0.14459244 0.\n",
            "  0.14459244 0.         0.10287871 0.         0.28918488 0.\n",
            "  0.         0.         0.         0.         0.41151485 0.\n",
            "  0.10287871 0.10287871 0.10287871 0.         0.28918488 0.\n",
            "  0.14459244 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Import: We import TfidfVectorizer from sklearn.feature_extraction.text.\n",
        "Sample Data: We create a list of sample text documents.\n",
        "Create Vectorizer: We instantiate a TfidfVectorizer object, specifying max_features=5 to extract the top 5 most informative features (you can adjust this parameter).\n",
        "Transform Documents: The fit_transform method both fits the vectorizer to the data (learns the vocabulary) and transforms the documents into TF-IDF vectors (numerical features).\n",
        "Print Results: We print the extracted feature names and their corresponding values in the TF-IDF vectors.\n",
        "This code demonstrates how TfidfVectorizer helps convert textual data into a numerical representation that machine learning models can understand and process effectively."
      ],
      "metadata": {
        "id": "Gz1lJPtdJI1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "IV32yU3JKUDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Importing from sklearn.metrics:\n",
        "\n",
        "This line imports two commonly used evaluation metrics from the sklearn.metrics module:\n",
        "\n",
        "accuracy_score: This function calculates the accuracy, which is the proportion of correctly classified samples. It's a basic metric but can be misleading in certain cases (e.g., imbalanced class distributions).\n",
        "f1_score: This function calculates the F1-score, a harmonic mean of precision and recall. It's often a more robust metric than accuracy, especially for imbalanced datasets.\n",
        "How They Work:\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "It's calculated as the number of correctly predicted samples divided by the total number of samples:"
      ],
      "metadata": {
        "id": "FhgSIMf5KVWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy = (True Positives + True Negatives) / (Total Samples)"
      ],
      "metadata": {
        "id": "7KKsiXCcKZPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1-score:\n",
        "\n",
        "It's the harmonic mean of precision and recall, where:\n",
        "\n",
        "Precision: Proportion of true positives among predicted positives.\n",
        "Recall: Proportion of true positives among actual positives.\n",
        "F1-score considers both precision and recall, making it a more balanced measure of a model's performance. It's calculated as:"
      ],
      "metadata": {
        "id": "yPCsTTjFKa5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1-score = 2 * (Precision * Recall) / (Precision + Recall)"
      ],
      "metadata": {
        "id": "uaJbcpT-KjMA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Sample predictions (replace with your actual predictions)\n",
        "y_true = [1, 0, 1, 0, 1]  # True labels\n",
        "y_pred = [0, 0, 1, 1, 0]  # Predicted labels\n",
        "\n",
        "# Calculate accuracy and F1-score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")  # Weighted average for multi-class\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-score (weighted): {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgKYRq--KmYw",
        "outputId": "1441feb9-b814-4648-f7d5-fc018300b75c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4000\n",
            "F1-score (weighted): 0.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code:\n",
        "\n",
        "We import accuracy_score and f1_score from sklearn.metrics.\n",
        "We define sample lists for true labels (y_true) and predicted labels (y_pred).\n",
        "We calculate accuracy using accuracy_score.\n",
        "We calculate F1-score using f1_score with the average parameter set to \"weighted\" (suitable for multi-class classification).\n",
        "We print the accuracy and F1-score values.\n",
        "In Conclusion:\n",
        "\n",
        "accuracy_score and f1_score provide essential tools for evaluating the performance of your machine learning models. While accuracy is a basic starting point, F1-score often offers a more comprehensive assessment, especially for imbalanced datasets. Remember to choose the metric(s) that best align with your specific problem and its evaluation criteria."
      ],
      "metadata": {
        "id": "RT4hm7wYKp7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "emails = [\"Buy this new product!\", \"Important meeting tomorrow\", \"Free money! Click here!\", \"Company update\"]\n",
        "labels = [1, 0, 1, 0]  # 1: Spam, 0: Not Spam\n",
        "\n",
        "# Feature extraction: Count occurrences of words in subject lines\n",
        "vectorizer = CountVectorizer()\n",
        "features = vectorizer.fit_transform(emails)\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic regression model for classification\n",
        "model = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example prediction on a new email\n",
        "new_email = \"hello my name is yke!\"\n",
        "new_features = vectorizer.transform([new_email])  # Transform new email\n",
        "prediction = model.predict(new_features)[0]\n",
        "\n",
        "if prediction == 1:\n",
        "  print(f\"Prediction: This email is likely spam.\")\n",
        "else:\n",
        "  print(f\"Prediction: This email is likely not spam.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_emK0BTL0De",
        "outputId": "30015a69-732e-45db-caf5-ea782d66a349"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0000\n",
            "Prediction: This email is likely spam.\n"
          ]
        }
      ]
    }
  ]
}